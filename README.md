# StyleGAN-NADA Inversion & Stylization Toolkit

### About the Project
This project provides a complete pipeline for image inversion and stylization using custom-trained StyleGAN-NADA generators. It automates model setup, preprocessing, encoding, and style transfer using CLIP-guided latent manipulation. The project is a re-implementation of StyleGAN-NADA according to the article: [https://arxiv.org/pdf/2108.00946](https://arxiv.org/pdf/2108.00946)

![](img/stylegan_nada.png)

### Features
* **Text-Guided Fine-Tuning:** Adapt a StyleGAN2 generator to a new domain using only text prompts, such as "a painting" or "a pencil sketch".
* **Directional CLIP Loss:** A custom loss function aligns the direction of image changes in CLIP space with the direction of text changes, enabling precise semantic control.
* **Dynamic Loss Coefficient Adjustment:** An independent optimizer automatically adjusts the loss weights ($\lambda_{CLIP}$ and $\lambda_{L2}$) to balance style transformation with content preservation.
* **Adaptive Layer Freezing:** An innovative method to automatically identify and unfreeze only the most relevant layers of the generator for a given task, improving training efficiency and result quality.
* **Real Image Editing:** The method can be combined with image inversion techniques to edit real-world images.

### Training Process
Training involves two generator copies:
* A **frozen generator** that always produces baseline images from the source domain.
* A **trainable generator** initialized identically to the frozen one, but updated to match the new domain.

CLIP is used to guide the direction of editing. The idea is to align the CLIP-direction (difference in embeddings) between images generated by the two generators to the direction between the source and target text prompts.

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uko3/StyleGAN-nada/tree/main/notebooks/01_Train_StyleGAN_NADA.ipynb) `01_Train_StyleGAN_NADA.ipynb`

### Experiments and Findings
Our research explored several key aspects of the StyleGAN-NADA method:
1.  **Adaptive vs. Static Layer Freezing:**
2.  **Granular Freezing Techniques:**
3.  **Dynamic Source Text Selection:**
4.  **Dynamic Loss Weighting:**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uko3/StyleGAN-nada/tree/main/notebooks/03_Experiments_StyleGAN_NADA.ipynb) `03_Experiments_StyleGAN_NADA.ipynb`

---

### More details
* Detailed report with experiments and conclusions: [Report StyleGAN-nada en.pdf](Report StyleGAN-nada en.pdf)

### Getting Started
**Requirements**
* Google Colaboratory (recommended)
* Access to GPU (Colab free tier or Pro)
* Runtime -> Run all
* All required pre-trained models will be downloaded automatically
* **Important:** Make sure to update the Google Drive IDs in the `stylegan_nada_generators` dictionary to load your custom generators.

**Examples:**
![](img/styles_img.png)

### Preparing Images for Inversion
1.  Place your `.png`, `.jpg`, or `.jpeg` images into the following folder:
2.  You can drag and drop images directly into the Colab file panel.

![](img/inferens_real_img.png)

### How It Works
After running all the cells:
* A few random samples will be generated using the first available generator
* All images found in `/data/inversion/` will be processed:
    * Face alignment -> Latent encoding -> Stylization
    * Visual comparison between original and stylized images will be displayed

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uko3/StyleGAN-nada/tree/main/notebooks/02_Inference_StyleGAN_NADA.ipynb) `02_Inference_StyleGAN_NADA.ipynb`

### Conclusion
Demonstrated that StyleGAN-NADA is a powerful framework for zero-shot generative image editing. With only a text prompt, the generator can be adapted to synthesize images in new domains — including fantastical or artistic styles — without requiring target data.