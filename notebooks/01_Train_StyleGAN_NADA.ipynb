{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0CRi4SbbRpa"
      },
      "outputs": [],
      "source": [
        "# Установка зависимостей и клонирование репозитория\n",
        "!git clone https://github.com/rosinality/stylegan2-pytorch.git\n",
        "%cd stylegan2-pytorch\n",
        "!pip install Ninja\n",
        "!pip install git+https://github.com/openai/CLIP.git -q\n",
        "\n",
        "# Создание структуры папок\n",
        "%mkdir -p ../modules\n",
        "%mkdir -p ../pretrained_models\n",
        "%mkdir -p ../notebooks\n",
        "%mkdir -p ../data/inversion\n",
        "%mkdir -p ../dlib_models\n",
        "\n",
        "# Перемещаем файлы модулей\n",
        "!mv model.py ../modules/model.py\n",
        "!echo \"\" > ../modules/__init__.py # Создаем пустой __init__.py\n",
        "\n",
        "# ВНИМАНИЕ: Вам нужно будет вручную создать modules/losses.py, modules/trainer.py, modules/utils.py\n",
        "# и скопировать туда код, который я предоставил выше.\n",
        "# Если вы запускаете это в Colab, вам придется создать и заполнить эти файлы вручную в `/content/modules/`\n",
        "# Например, можно использовать %%writefile\n",
        "# %%writefile ../modules/losses.py\n",
        "# ... (код для losses.py) ...\n",
        "# %%writefile ../modules/trainer.py\n",
        "# ... (код для trainer.py) ...\n",
        "# %%writefile ../modules/utils.py\n",
        "# ... (код для utils.py) ...\n",
        "\n",
        "%cd ../ # Возвращаемся в корневую папку проекта, чтобы импорты работали правильно\n",
        "\n",
        "# Импорты\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "import copy\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "# Импортируем наши собственные модули\n",
        "from modules.model import Generator\n",
        "from modules.losses import CLIPLoss, CLIPDirectionalLoss\n",
        "from modules.trainer import LatentStyleTrainer\n",
        "from modules.utils import freeze_layers_adaptive, generate_visualize_and_save # Import freeze_layers_adaptive directly\n",
        "\n",
        "# Зададим девайс\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Используется устройство: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка предварительно обученной модели StyleGAN2\n",
        "# Если файл уже скачан, пропустите это\n",
        "if not os.path.exists('pretrained_models/stylegan2-ffhq-config-f.pt'):\n",
        "    !gdown https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT -O pretrained_models/stylegan2-ffhq-config-f.pt\n",
        "\n",
        "# Параметры генератора\n",
        "size = 1024\n",
        "latent_dim = 512\n",
        "n_mlp = 8\n",
        "channel_multiplier = 2\n",
        "ckpt_path = 'pretrained_models/stylegan2-ffhq-config-f.pt'\n",
        "\n",
        "# Инициализация генератора\n",
        "generator = Generator(size, latent_dim, n_mlp, channel_multiplier=channel_multiplier).to(device)\n",
        "generator.eval()\n",
        "checkpoint = torch.load(ckpt_path)\n",
        "generator.load_state_dict(checkpoint[\"g_ema\"])"
      ],
      "metadata": {
        "id": "ET0uXXTFdK7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_class = \"Photo\"\n",
        "target_class = \"Sketch\"\n",
        "text_source = clip.tokenize([source_class]).to(device)\n",
        "text_target = clip.tokenize([target_class]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_features_source = model_clip.encode_text(text_source)\n",
        "    text_features_target = model_clip.encode_text(text_target)\n",
        "text_features_source = text_features_source / text_features_source.norm(dim=-1, keepdim=True)\n",
        "text_features_target = text_features_target / text_features_target.norm(dim=-1, keepdim=True)\n",
        "sim = torch.nn.functional.cosine_similarity(text_features_target, text_features_source)\n",
        "print(f\"Text sim: {sim.item():.4f}\")\n",
        "\n",
        "# Инициализация CLIP Directional Loss\n",
        "clip_directional_loss_fn = CLIPDirectionalLoss()"
      ],
      "metadata": {
        "id": "Abdel4LWdAoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация тренера\n",
        "trainer = LatentStyleTrainer(\n",
        "    generator=generator,\n",
        "    model_clip=model_clip,\n",
        "    text_features_source=text_features_source,\n",
        "    text_features_target=text_features_target,\n",
        "    freeze_fn=lambda model_train, model_frozen, text_target_feat, top_k: freeze_layers_adaptive(\n",
        "        model_train, model_frozen, text_target_feat, k=top_k, device=device\n",
        "    ), # Pass text_target_feat and device\n",
        "    clip_directional_loss=clip_directional_loss_fn,\n",
        "    latent_dim=latent_dim,\n",
        "    batch_size=2, # Важно, чтобы batch_size был >= 2 для directional loss\n",
        "    device=device,\n",
        "    lr_generator=0.0008,\n",
        "    lr_lambda=0.01, # Немного уменьшил LR для lambda для большей стабильности\n",
        "    weight_decay=0.003,\n",
        "    lambda_clip_init=5.0, # Установлено 5.0 для соответствия вашим логам\n",
        "    lambda_l2_init=0.2, # Установлено 0.2 для соответствия вашим логам\n",
        ")\n",
        "\n",
        "# Запуск обучения\n",
        "epochs_to_train = 61 # или другое значение\n",
        "trainer.train(epochs=epochs_to_train, freeze_each_epoch=True, reclassify=False)"
      ],
      "metadata": {
        "id": "FfiFpAbyc5_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Построение графиков потерь\n",
        "trainer.plot_losses()\n",
        "\n",
        "# Визуализация и сохранение финальных изображений\n",
        "seeds = (92126, 773, 779, 373, 2112) # Используйте те же сиды, что и в вашем коде\n",
        "generate_visualize_and_save(trainer, seeds, output_dir=\"../validation_outputs\", folder_name=\"sketch\")\n",
        "\n",
        "# Визуализация направлений CLIP\n",
        "latent_w_vis = trainer.sample_latent_w(seed=seeds[0]) # Используем первый сид для визуализации\n",
        "image_frozen_vis, _ = trainer.model[\"generator_frozen\"]([latent_w_vis], input_is_latent=True, randomize_noise=False)\n",
        "image_styled_vis, _ = trainer.model[\"generator_train\"]([latent_w_vis], input_is_latent=True, randomize_noise=False)\n",
        "trainer.visualize_clip_directions(image_frozen=image_frozen_vis, image_styled=image_styled_vis,\n",
        "                                  text_target=text_target, text_source=text_source, preprocess=preprocess)"
      ],
      "metadata": {
        "id": "gWI4G37kcp4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение обученной модели\n",
        "output_model_path = \"pretrained_models/my_custom_stylegan_generator.pth\"\n",
        "trainer.model[\"generator_train\"].eval()\n",
        "torch.save(trainer.model[\"generator_train\"].state_dict(), output_model_path)\n",
        "print(f\"Модель сохранена в: {output_model_path}\")"
      ],
      "metadata": {
        "id": "Ps-dm2zrcmBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}