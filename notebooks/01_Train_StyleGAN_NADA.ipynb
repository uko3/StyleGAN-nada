{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/uko3/StyleGAN-nada.git\n!pip install Ninja\n!pip install git+https://github.com/openai/CLIP.git -q\n%cd StyleGAN-nada","metadata":{"id":"AgJn8DrunUuZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Импорты\nimport torch\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport math\nimport copy\nimport re\nimport warnings\nimport clip\n\n# Импортируем собственные модули\nfrom modules.stylegan_arch.model import Generator\nfrom modules.losses import CLIPLoss, CLIPDirectionalLoss\nfrom modules.trainer import LatentStyleTrainer\nfrom modules.utils import freeze_layers_adaptive, generate_visualize_and_save # Import freeze_layers_adaptive directly\n\n# Зададим девайс\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Используется устройство: {device}\")","metadata":{"id":"t0CRi4SbbRpa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Загрузка предварительно обученной модели StyleGAN2\nif not os.path.exists('stylegan2-ffhq-config-f.pt'):\n    !gdown https://drive.google.com/uc?id=1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT -O stylegan2-ffhq-config-f.pt\n\n# Параметры генератора\nsize = 1024\nlatent_dim = 512\nn_mlp = 8\nchannel_multiplier = 2\nckpt_path = 'stylegan2-ffhq-config-f.pt'\n\n# Инициализация генератора\ngenerator = Generator(size, latent_dim, n_mlp, channel_multiplier=channel_multiplier).to(device)\ngenerator.eval()\ncheckpoint = torch.load(ckpt_path)\ngenerator.load_state_dict(checkpoint[\"g_ema\"])","metadata":{"id":"ET0uXXTFdK7A","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_clip, preprocess = clip.load('ViT-B/32', device)","metadata":{"id":"r6IXfGjqtM-o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import clip\nsource_class = \"photo\"\ntarget_class = \"sketch\" #plasticine\ntext_source = clip.tokenize([source_class]).to(device)\ntext_target = clip.tokenize([target_class]).to(device)\n\nwith torch.no_grad():\n    text_features_source = model_clip.encode_text(text_source)\n    text_features_target = model_clip.encode_text(text_target)\ntext_features_source = text_features_source / text_features_source.norm(dim=-1, keepdim=True)\ntext_features_target = text_features_target / text_features_target.norm(dim=-1, keepdim=True)\nsim = torch.nn.functional.cosine_similarity(text_features_target, text_features_source)\nprint(f\"Text sim: {sim.item():.4f}\")\n\n# Инициализация CLIP Directional Loss\nclip_directional_loss_fn = CLIPDirectionalLoss()\n","metadata":{"id":"Abdel4LWdAoh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_k = 10\n# Инициализация тренера\ntrainer = LatentStyleTrainer(\n    generator=generator,\n    model_clip=model_clip,\n    text_features_source=text_features_source,\n    text_features_target=text_features_target,\n    freeze_fn=lambda model_train, model_frozen, text_target_feat: freeze_layers_adaptive(\n        model_train, model_frozen, text_target_feat, k=top_k, device=device\n    ), \n    clip_directional_loss=clip_directional_loss_fn,\n    latent_dim=latent_dim,\n    batch_size=2,\n    device=device,\n    lr_generator=0.008,\n    lr_lambda=0.02, \n    weight_decay=0.003,\n    lambda_clip_init=1.0, # Установлено 5.0 для соответствия вашим логам\n    lambda_l2_init=1.0, # Установлено 0.2 для соответствия вашим логам\n)\n\n# Запуск обучения\nepochs_to_train = 41\ntrainer.train(epochs=epochs_to_train, freeze_each_epoch=True, reclassify=False)","metadata":{"id":"FfiFpAbyc5_O","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Построение графиков потерь\ntrainer.plot_losses()\n\n# Визуализация и сохранение финальных изображений\nseeds = (92126, 773, 779, 373, 2112)\ngenerate_visualize_and_save(trainer, seeds, output_dir=\"../validation_outputs\", folder_name=\"sketch\")\n\n# Визуализация направлений CLIP\nlatent_w_vis = trainer.sample_latent_w(seed=seeds[0]) # Используем первый сид для визуализации\nimage_frozen_vis, _ = trainer.model[\"generator_frozen\"]([latent_w_vis], input_is_latent=True, randomize_noise=False)\nimage_styled_vis, _ = trainer.model[\"generator_train\"]([latent_w_vis], input_is_latent=True, randomize_noise=False)\ntrainer.visualize_clip_directions(image_frozen=image_frozen_vis, image_styled=image_styled_vis,\n                                  text_target=text_target, text_source=text_source, preprocess=preprocess)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сохранение обученной модели\n# output_model_path = \"pretrained_models/my_custom_stylegan_generator.pth\"\n\noutput_model_path = \"pretrained_models/\"+target_class+\".pth\"\ntrainer.model[\"generator_train\"].eval()\ntorch.save(trainer.model[\"generator_train\"].state_dict(), output_model_path)\nprint(f\"Модель сохранена в: {output_model_path}\")","metadata":{"id":"Ps-dm2zrcmBP","trusted":true},"outputs":[],"execution_count":null}]}